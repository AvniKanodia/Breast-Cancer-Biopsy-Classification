{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyiDunLtmcem8Mojqsht33"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Breast Cancer Biopsy Classification\n",
        "The aim of this project is to build a classifier that can determine whether a breast cancer biopsy sample is malignant or benign.\n",
        "\n",
        "Every iopsy sample has various metrics are recorded about it, including: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension.\n",
        "\n",
        "Using a large dataset of labeled biopsy samples from breast cancer tumors, I will try to build a binary classification model to determine whether a tumor is malignant or benign based on these features. \n",
        "\n",
        "In this project I will use mutliple approches to build a model:\n",
        "\n",
        "*   Approach 1: Linear Regression \n",
        "*   Approach 2: Boundary classifier\n",
        "*   Approach 3: Logistic Regression\n",
        "*   Approach 4: Multiple Feature Logistic Regression\n",
        "*   Approach 5: Decision Tree Model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gboAKA9vvum6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download Data**\n",
        "\n",
        "The dataset used to train these models is called the Breast Cancer Wisconsin (Diagnostic) Data Set. It consists of 569 biopsy samples, just like the ones described above, from breast cancer tumors.\n",
        "\n",
        "Each biopsy sample in the dataset is labeled with an ID number and whether or not the tumor it came from is malignant (1) or benign (0). Each sample also has 10 different features associated with it, some of which are described above. "
      ],
      "metadata": {
        "id": "33G4WzmMv1GK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwz43qVSvrzH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%202b%20-%20Logistic%20Regression/cancer.csv\"\n",
        "\n",
        "data = pd.read_csv('cancer.csv')\n",
        "data['diagnosis'].replace({'M':1, 'B':0}, inplace = True)\n",
        "data.to_csv('cancer.csv')\n",
        "del data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Feature Descriptions**\n",
        "1. 𝑃𝑒𝑟𝑖𝑚𝑒𝑡𝑒𝑟 : Total distance between points defining the cell's nuclear perimeter.\n",
        "2. 𝑅𝑎𝑑𝑖𝑢𝑠 : Average distance from the center of the cell's nucleus to its perimeter.\n",
        "3. 𝑇𝑒𝑥𝑡𝑢𝑟𝑒 : The texture of the cell nucleus is measured by finding the variance of the gray scale intensities in the component pixels.\n",
        "4. 𝐴𝑟𝑒𝑎 : Nuclear area is measured by counting the number of pixels on the interior of the nucleus and adding one-half of the pixels in the perimeter.\n",
        "5. 𝑆𝑚𝑜𝑜𝑡ℎ𝑛𝑒𝑠𝑠 : Measures the smoothness of a nuclear contour by measuring the difference between the length of a radial line and the mean length of the lines surrounding it.\n",
        "6. 𝐶𝑜𝑛𝑐𝑎𝑣𝑖𝑡𝑦 : Measures the severity of concavities or indentations in a cell nucleus. Chords are drawn between non-adjacent snake points and measure the extent to which the actual boundary lies inside each chord.\n",
        "7. 𝑆𝑦𝑚𝑚𝑒𝑡𝑟𝑦 : The major axis (longest chord) through the center is found. Then, the difference between the distance on both sides of the lines that are perpendicular to the major axis is calculated. The image below shows an example of this:\n",
        "\n",
        "The paper that first detailed these measurements for this dataset can be found here for more information: https://pdfs.semanticscholar.org/1c4a/4db612212a9d3806a848854d20da9ddd0504.pdf"
      ],
      "metadata": {
        "id": "Y4TMrdro_OS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading annotated dataset"
      ],
      "metadata": {
        "id": "7aNFPgmPwEWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Python tools for loading/navigating data\n",
        "import os             # Good for navigating your computer's files \n",
        "import numpy as np    # Great for lists (arrays) of numbers\n",
        "import pandas as pd   # Great for tables (google spreadsheets, microsoft excel, csv)\n",
        "from sklearn.metrics import accuracy_score   # Great for creating quick ML models"
      ],
      "metadata": {
        "id": "ONWrxazgwGuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path  = 'cancer.csv'\n",
        "\n",
        "dataframe = pd.read_csv(data_path)\n",
        "\n",
        "dataframe = dataframe[['diagnosis', 'perimeter_mean', 'radius_mean', 'texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean', 'symmetry_mean']]\n",
        "dataframe['diagnosis_cat'] = dataframe['diagnosis'].astype('category').map({1: '1 (malignant)', 0: '0 (benign)'})"
      ],
      "metadata": {
        "id": "xhe7tQKTwRRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing dataset"
      ],
      "metadata": {
        "id": "kPI7nOFLwZkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt "
      ],
      "metadata": {
        "id": "AiBtn6FhwbCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.catplot(x = 'radius_mean', y = 'diagnosis_cat', data = dataframe, order=['1 (malignant)', '0 (benign)'])\n",
        "dataframe.head()"
      ],
      "metadata": {
        "id": "Y4DZ1vLEwc-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Diagnosis\n",
        "\n",
        "Satring predicting a diagnosis using a single feature: mean radius"
      ],
      "metadata": {
        "id": "cA2WfQ96wkYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Approach: Linear Regression\n",
        "\n",
        "Fitting and visualizing a linear regression:"
      ],
      "metadata": {
        "id": "XsuyCYWVwxmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and visualize a linear regression \n",
        "from sklearn import linear_model\n",
        "\n",
        "X,y = dataframe[['radius_mean']], dataframe[['diagnosis']]\n",
        "\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(X, y)\n",
        "preds = model.predict(X)\n",
        "\n",
        "sns.scatterplot(x='radius_mean', y='diagnosis', data=dataframe)\n",
        "plt.plot(X, preds, color='r')\n",
        "plt.legend(['Linear Regression Fit', 'Data'])"
      ],
      "metadata": {
        "id": "KjrzP6KPw_He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Approach: Boundary Classifier\n",
        "The variable we are trying to predict is categorical, not continuous. So, we can't use a linear regression; we have to use a classifier."
      ],
      "metadata": {
        "id": "AKpMnvF8xisr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boundary = 15 # change me!\n",
        "\n",
        "sns.catplot(x = 'radius_mean', y = 'diagnosis_cat', data = dataframe, order=['1 (malignant)', '0 (benign)'])\n",
        "plt.plot([boundary, boundary], [-.2, 1.2], 'g', linewidth = 2)"
      ],
      "metadata": {
        "id": "iZsf6-bFxxJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the boundary Classifier:\n",
        "\n",
        "This function will take in a boundary value of our choosing and then classify the data points based on whether or not they are above or below the boundary."
      ],
      "metadata": {
        "id": "cP1DNzamxi17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boundary_classifier(target_boundary, radius_mean_series):\n",
        "  result = []\n",
        "  for i in radius_mean_series:\n",
        "    if i > target_boundary:\n",
        "      result.append(1)\n",
        "    else:\n",
        "      result.append(0)\n",
        "  return result"
      ],
      "metadata": {
        "id": "rD43OkXKx_Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the classifier"
      ],
      "metadata": {
        "id": "DANM9RPkyE12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_boundary = 15\n",
        "\n",
        "y_pred = boundary_classifier(chosen_boundary, dataframe['radius_mean'])\n",
        "dataframe['predicted'] = y_pred\n",
        "\n",
        "y_true = dataframe['diagnosis']\n",
        "\n",
        "sns.catplot(x = 'radius_mean', y = 'diagnosis_cat', hue = 'predicted', data = dataframe, order=['1 (malignant)', '0 (benign)'])\n",
        "plt.plot([chosen_boundary, chosen_boundary], [-.2, 1.2], 'g', linewidth = 2)"
      ],
      "metadata": {
        "id": "yVmQfT21yGfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating accuracy"
      ],
      "metadata": {
        "id": "431XvPk_yMCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_true,y_pred)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "X4rpb1dJyOGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Third Approach: Logistic Regression\n",
        "Splitting Traning and Testing Data\n"
      ],
      "metadata": {
        "id": "ALhD3HhmydVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(dataframe, test_size = 0.2, random_state = 1)"
      ],
      "metadata": {
        "id": "d9ZKXGyPypC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single Variable Logistic Regression:\n",
        "\n",
        "Building a logistic regression model to predict the diagnosis using radius mean"
      ],
      "metadata": {
        "id": "lSrTVqjByvgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = ['radius_mean']\n",
        "y = 'diagnosis'\n",
        "\n",
        "X_train = train_df[X]\n",
        "print('X_train, our input variables:')\n",
        "print(X_train.head())\n",
        "print()\n",
        "\n",
        "y_train = train_df[y]\n",
        "print('y_train, our output variable:')\n",
        "print(y_train.head())"
      ],
      "metadata": {
        "id": "Sp2vS8aey11A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the model"
      ],
      "metadata": {
        "id": "CrP2x-Xey64x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_model = linear_model.LogisticRegression()\n",
        "logreg_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "0B1vORQIy8k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Model"
      ],
      "metadata": {
        "id": "2e5yZ1iizKWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_df[X]\n",
        "y_test = test_df[y]\n",
        "y_pred = logreg_model.predict(X_test)"
      ],
      "metadata": {
        "id": "F1NDO9sOzMlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the results"
      ],
      "metadata": {
        "id": "tHt8b9D3zUL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['predicted'] = y_pred.squeeze()\n",
        "sns.catplot(x = 'radius_mean', y = 'diagnosis_cat', hue = 'predicted', data=test_df, order=['1 (malignant)', '0 (benign)'])"
      ],
      "metadata": {
        "id": "9KK8d7M_zVnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the accuracy"
      ],
      "metadata": {
        "id": "NVTZmkE9zYOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "l-wEsCcJzaCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting logistic regeresison's soft probablity"
      ],
      "metadata": {
        "id": "CXjoYhoezldi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_prob = logreg_model.predict_proba(X_test)\n",
        "X_test_view = X_test[X].values.squeeze()\n",
        "plt.xlabel('radius_mean')\n",
        "plt.ylabel('Predicted Probability')\n",
        "sns.scatterplot(x = X_test_view, y = y_prob[:,1], hue = y_test, palette=['purple','green'])"
      ],
      "metadata": {
        "id": "2knQYy5IzsF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fourth Approach: Multiple Feature Logistic Regression\n",
        "Using more feautres in the logistic regression to predict diagnosis"
      ],
      "metadata": {
        "id": "Qq9hcYkjzx_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_X = ['perimeter_mean', 'radius_mean', 'texture_mean','area_mean']\n",
        "y = 'diagnosis'\n",
        "\n",
        "# 1. Split data into train and test\n",
        "multi_train_df, multi_test_df = train_test_split(dataframe, test_size = 0.2, random_state = 1)\n",
        "\n",
        "# 2. Prepare your X_train, X_test, y_train, and y_test variables by extracting the appropriate columns:\n",
        "multi_X_train, multi_X_test = multi_train_df[multi_X], multi_test_df[multi_X]\n",
        "y_train, y_test = multi_train_df[y], multi_test_df[y]\n",
        "\n",
        "# 3. Initialize the model object\n",
        "model = linear_model.LogisticRegression()\n",
        "\n",
        "# 4. Fit the model to the training data\n",
        "model.fit(multi_X_train, y_train)\n",
        "\n",
        "# 5. Use this trained model to predict on the test data\n",
        "multi_preds = model.predict(multi_X_test)\n",
        "\n",
        "# 6. Evaluate the accuracy by comparing to to the test labels and print out accuracy.\n",
        "accuracy = accuracy_score(y_test, multi_preds)\n",
        "print(multi_X)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "gJuiNV_X0AlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression can learn an optimal classification boundary by using multiple features together, which can improve its prediction accuracy even more."
      ],
      "metadata": {
        "id": "Ia2BODjH0i6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a confusion matrix"
      ],
      "metadata": {
        "id": "INH39vhs0dBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the metrics class\n",
        "from sklearn import metrics\n",
        "\n",
        "# Create the Confusion Matrix\n",
        "# y_test = dataframe['diagnosis']\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualizing the Confusion Matrix\n",
        "class_names = [0,1] # Our diagnosis categories\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "# Setting up and visualizing the plot (do not worry about the code below!)\n",
        "tick_marks = np.arange(len(class_names)) \n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g') # Creating heatmap\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix', y = 1.1)\n",
        "plt.ylabel('Actual diagnosis')\n",
        "plt.xlabel('Predicted diagnosis')"
      ],
      "metadata": {
        "id": "aiuji7J70fVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating `True Negative`, `False Positive`, `False Negative` and `True Postive` metrics from confusion matrix"
      ],
      "metadata": {
        "id": "jivg1pyT07DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (cnf_matrix)\n",
        "(tn, fp), (fn, tp) = cnf_matrix\n",
        "print (\"TN, FP, FN, TP:\", tn, fp, fn, tp)"
      ],
      "metadata": {
        "id": "0nMIOteL0rYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate model's performace by chosen metric"
      ],
      "metadata": {
        "id": "fr0iHVBm1QKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = (tp + tn)/(tn + fp + fn + tp)\n",
        "precision = (tp)/(tp + fp)\n",
        "recall = tp/(tp + fn)\n",
        "\n",
        "print (\"accuracy, precision, recall\", accuracy, precision, recall)"
      ],
      "metadata": {
        "id": "QvLpnco00wwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fifth Approach: Decision Tree Model"
      ],
      "metadata": {
        "id": "h5l0Y1iu1YR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model"
      ],
      "metadata": {
        "id": "qrYvNIWb1dSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "\n",
        "class_dt = tree.DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "# Use previous `X_train` and `y_train` sets to build the model\n",
        "class_dt.fit(multi_X_train, y_train)"
      ],
      "metadata": {
        "id": "XZLaURZ71eZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize and interpret the tree"
      ],
      "metadata": {
        "id": "c0rYxGHb1nSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(13,8))  # set plot size\n",
        "tree.plot_tree(class_dt, fontsize=10) "
      ],
      "metadata": {
        "id": "FgOxa8VS1pki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find predictions based on model"
      ],
      "metadata": {
        "id": "n3AOgaTF1stf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_y_pred = class_dt.predict(multi_X_test)"
      ],
      "metadata": {
        "id": "diqRAgxA1u_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate model preformance"
      ],
      "metadata": {
        "id": "G0l5RhTa1xg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", metrics.accuracy_score(y_test, multi_y_pred))\n",
        "print(\"Precision: \", metrics.precision_score(y_test, multi_y_pred))\n",
        "print(\"Recall: \", metrics.recall_score(y_test, multi_y_pred))"
      ],
      "metadata": {
        "id": "4Sc_ZQgZ1zK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing a Classifier\n",
        "Let's try to choose the overall best classifier for this dataset. I will:\n",
        "\n",
        "*   Use a for loop to train and evaluate each classifer in the list on our dataset.\n",
        "*   Calculate the precision, recall, and accuracy on the test set for each classifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JhdSg4oy18l1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import classifiers"
      ],
      "metadata": {
        "id": "iTS8tvq12Z7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
      ],
      "metadata": {
        "id": "W9a908xq2Wsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a for loop to train and test each classifier, and print the results"
      ],
      "metadata": {
        "id": "C9DMpkpK2qX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel=\"linear\", C=0.025),\n",
        "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
        "    DecisionTreeClassifier(max_depth=5),\n",
        "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "    MLPClassifier(alpha=1, max_iter=1000),\n",
        "    AdaBoostClassifier(),\n",
        "    GaussianNB(),\n",
        "    QuadraticDiscriminantAnalysis()] \n",
        "\n",
        "for classifier in classifiers:\n",
        "  print(\"---------------\")\n",
        "  print(str(classifier) + '\\n')\n",
        "  classifier.fit(multi_X_train, y_train)\n",
        "  multi_y_pred = classifier.predict(multi_X_test)\n",
        "  print(\"Accuracy: \", metrics.accuracy_score(y_test, multi_y_pred))\n",
        "  print(\"Precision: \", metrics.precision_score(y_test, multi_y_pred))\n",
        "  print(\"Recall: \", metrics.recall_score(y_test, multi_y_pred)) \n",
        "  print(\"---------------\")"
      ],
      "metadata": {
        "id": "6Z3gxAYW2f1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more successful hyperparameters for your classifiers. Your experiments will help you find a classifier that works very well on our test set. However, you're running a risk by doing so much manual fine-tuning: you might end up \"overfitting\" by choosing a classifier that works well on your test set, but might not work well on other data.\n",
        "\n",
        "That's why remember to have a training set that we use to train each candidate model; a validation set that we use to evaluate each candidate model and choose the best one; and finally, a test set which we use only once, if you choose to experiment a lot with the hyperparamters"
      ],
      "metadata": {
        "id": "-K0etdDg3mj6"
      }
    }
  ]
}